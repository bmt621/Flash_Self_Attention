# Flash_Self_Attention
This is my implementation of self attention model that both support pytorch efficient fused kernel flash attention and normal self attention 
